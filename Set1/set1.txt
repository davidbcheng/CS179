CS 179: GPU Computing
Assignment 1

Due: Wednesday, April 8, 2015 - 3:00 PM

Submission: 

Include your written answers in a readme file. Submit these answers, as well as 
your code, by e-mail to kyuh@caltech.edu. Package your files in a standard 
archive format (e.g. zip, tar.gz, tar.bz2).



Question 1: Common errors (20pts)
--------------------------------------------------------
--------------------------------------------------------

1.1
---------------------

Creates an integer pointer, sets the value to which it points to 3, adds 2 to 
this value, and prints said value.


void test1(){
    int *a = 3;
    *a = *a + 2;
    printf("%d",*a);
}

The problem with test1 is in the int* a = 3; line. In the current line, we
are setting the integer pointer a to point to memory address 3. We cannot
write to memory address 3 so this will cause a segmentation fault. To fix
the implementation, we need to either malloc memory for an int on the heap and
write the value of 3 into that memory address or define the value of 3 onto
the stack and set the integer pointer a to the address of the value.

Heap:
void test1(){
    int *a = (int *) malloc(sizeof(int));
    *a = 3;
    *a = *a + 2;
    printf("%d",*a);
}

Stack:
void test1() {
	int temp = 3;
	int *a = &temp;
	*a = *a + 2;
	printf("%d",*a);
}


1.2
---------------------

Creates two integer pointers and sets the values to which they point to 2 and 3,
 respectively.


void test2(){
    int* a,b;
    a = (int*) malloc(sizeof(int));
    b = (int*) malloc(sizeof(int));

    if (!(a && b)){
        printf("Out of memory");
        exit(-1);
    }
    *a = 2;
    *b = 3;
}

In the first line of the function, we are trying to create two integer pointers.
The current line declares an integer pointer a and an integer b. The multiple
declaraction rule corrected is as follows:

void test2(){
    int *a, *b;
    a = (int*) malloc(sizeof(int));
    b = (int*) malloc(sizeof(int));

    if (!(a && b)){
        printf("Out of memory");
        exit(-1);
    }
    *a = 2;
    *b = 3;
}

1.3 
---------------------

Allocates an array of 1000 integers, and for i = 0,...,999, sets the i-th
 element to i.


void test3(){
    int i, *a = (int*) malloc(1000);

    if (!a){
        printf("Out of memory");
        exit(-1);
    }
    for (i = 0; i < 1000; i++)
        *(i+a)=i;
}

In the first line of the function, we are allocating 1000 bytes on the heap. 
However, we want an array of 1000 elements, so we want to allocate 1000 *
the size of an integer. The corrected function is:

void test3(){
    int i, *a = (int*) malloc(1000 * sizeof(int));

    if (!a){
        printf("Out of memory");
        exit(-1);
    }
    for (i = 0; i < 1000; i++)
        *(i+a)=i;
}


1.4 
---------------------

Creates a two-dimensional array of size 3x100, and sets element (1,1) (counting
 from 0) to 5.


void test4(){
    int **a = (int**) malloc(3*sizeof(int*));
    a[1][1] = 5;
}

The problem with the above code is that we are allocating the memory for the 
pointers to pointers (3), but we also need to allocate memory for each array
within each pointer (100). The corrected code is as follows. For each of the
three int **, we need to allocate space for 100 ints. 

void test4(){
    int **a = (int**) malloc(3*sizeof(int*));
    int i;

    if (!a) {
    	printf("Out of memory");
        exit(-1);
    }

    for(i = 0; i < 3; i++) {
    	a[i] = (int *) malloc(100 * sizeof(int*));
        if(!(a[i])) {
            printf("Out of memory");
            exit(1);
        }
    }

    a[1][1] = 5;
}

1.5
---------------------

Sets the value pointed to by a to an input, checks if the value pointed to by a 
is 0, and prints a message if it is.


void test5(){
    int *a = (int*) malloc(sizeof(int));
    scanf("%d",a);
    if (!a)
        printf("Value is 0\n");
}

The problem is with the if statement. At its current state, the if statement
is checking if the pointer is a null pointer or not, instead of checking the
value at that memory address. To check the value pointed to by a (the value
inputted by the user), we need to dereference a by *a.

void test5(){
    int *a = (int*) malloc(sizeof(int));
    scanf("%d", a);
    if (!(*a))
        printf("Value is 0\n");
}


Question 2: Parallelization (30pts)
--------------------------------------------------------
--------------------------------------------------------


2.1
---------------------

Given an input signal x[n], suppose we have two output signals y_1[n] and y_2[n]
, given by the difference equations: 
		y_1[n] = x[n-1] + x[n] + x[n+1]
		y_2[n] = y_2[n-2] + y_2[n-1] + x[n]

Which calculation do you expect will have an easier and faster implementation on 
the GPU, and why?

The first output signal y_1[n] will have an easier and faster implementation 
on the GPU because it does not depend on previous output, so we can easily 
paralellize y_1[n] by separating each element's computation into different 
threads. We can send y_1[1], y_1[2], y_1[3], ... , y_1[n-1] each to a different
thread and run them in parallel because they do not depend on each other.

The second output signal y_2[n] will be harder to implement because it depends
on previous output. This is a serial computation.
Unlike the first output signal, we cannot separate
each elements computation because we have to each signal output depends on the
previous two output signals. For example, we need to calculate y_2[2] and
y_2[3] before we calculate y_2[4]. This suggests that we cannot separte 
the computatoin into different threads and run them in parallel.

2.2
---------------------

In class, we discussed how the exponential moving average (EMA), in comparison 
to the simple moving average (SMA), is much less suited for parallelization on 
the GPU. 

Recall that the EMA is given by:
	y[n] = c x[n] + (1-c) y[n-1]

Suppose that c is close to 1, and we only require an approximation to y[n]. How 
can we get this approximation in a way that is parallelizable? (Explain in words
 optionally along with pseudocode or equations.)


Hint: If c is close to 1, then 1-c is close to 0. If you expand the recurrence 
relation a bit, what happens to the contribution (to y[n]) of the terms y[n-k] 
as k increases?

The equation for the EMA is given by y[n] = c x[n] + (1-c) y[n-1]. We can 
subsitute y[n-1] = c x[n-1] + (1-c) y[n-2], so the relation is:

y[n] = c x[n] + (1-c) (c x[n-1] + (1-c)y[n-2]). 

If we expand out k for k terms we have that:

y[n] = c x[n] + c * (sum from i = 1 to k-1 of ((1-c)^i * x[n-i])) +
 (1-c)^k y[n-k]

The last term (1-c)^k y[n-k] is exponentially small since we are multiplying
it by many factors that are close to 0. We can approximate the solution y[n] by
getting rid of the last term that depends on previous output and so the 
solution y[n] is computed from the current x[n], x[n-1], x[n-2], ..., x[n-(k-1)]

This solution is parallelizable and is similar to the simple moving average,
which was shown in lecture to be parallelizable. The only difference is that
we are multiplying values by factors of either c or 1-c, which each process
on the gpu can handle.



Question 3: Small-kernel convolution (50pts)
--------------------------------------------------------
--------------------------------------------------------


Introduction:
------------------

On Friday, we saw that the effect of a linear time-invariant system on an input 
signal x[n] (to produce an output y[n]) can be summarized by the system's
 impulse response h[n]. This is the output of the system in response to a
  unit impulse as input.

We can then find y[n] by computing the convolution, which we denote (*):

	y[n] = (x (*) h)[n]

(See Friday's lecture slides for an expanded definition.)


The goal is to GPU-accelerate this computation. Similar to how we handled the 
addition problem, we allocate and copy memory as appropriate, and we can use 
the strategies in Lecture 2 to divide indicies among our many threads.


To do:
------------------
Complete the GPU-accelerated convolution by filling in the parts marked
 TODO in Blur.cc and Blur_cuda.cu .


Notes:
------------------
The code given to you will run the ordinary CPU version of the convolution, 
and compare the GPU/CPU speedup and the correctness of the GPU output. The 
default is currently set to convolve the starting signal with a Gaussian kernel.

There are two modes of operation:

	Normal mode: Generate the input signal x[n] randomly, with a size specified 
    in the arguments.

	Audio mode: Read the input signal x[n] from an input audio file, and write
     the signal y[n] as an output audio file.

To toggle between these two modes, set AUDIO_ON accordingly, and use the 
appropriate makefile.

Normal mode works on the servers haru, mx, and minuteman. 
Audio mode works only on haru.

Because convolving with the Gaussian kernel acts as an imperfect low-pass filter
, the output file (in audio mode) will have its higher frequencies attenuated. 
Try it out!


On haru (haru.caltech.edu), the expected runtime (assuming no other users) is 
about 30 ms on the GPU, and about 185 ms on the CPU (~6.2x speedup), using a
 reasonable choice of block size and #blocks (e.g. 512, 200). 

>> (revised 4/5/2015)
On haru (haru.caltech.edu), you should get a speedup of ~6-8x, using a
 reasonable choice of block size and #blocks (e.g. 512, 200). 


Hints:
------------------

- The CPU code exists already; use it as a guide! Recall that we often 
accelerate CPU code by replacing it with "similar-looking" GPU code!










